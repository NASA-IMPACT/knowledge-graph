{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Tiny Knowledge Graph with BERT and Graph Convolutions\n",
    "This notebook is the source of the code to build and test the graph described in the article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "from google.cloud import language\n",
    "import numpy\n",
    "import six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a simple function for cleaning and extracting sentences.   it was pulled from the web.  not sure of provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# invoking the google entity extractor\n",
    "to use it you must set up a google cloud account and sign up for the language.googleapis.com service.  You will also need your google credential set  up correctly.  in writing the article and debugging this notebook,  i have spent $26.   so if you have a free google cloud account it won't cost much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_client = language.LanguageServiceClient()\n",
    "\n",
    "def classify(text, verbose=True):\n",
    "    \"\"\"Classify the input text into categories. \"\"\"\n",
    "    document = language.Document(\n",
    "        content=text,\n",
    "        type_=language.Document.Type.PLAIN_TEXT\n",
    "    )\n",
    "    response = language_client.analyze_entities(request={ 'document': document })\n",
    "    return response\n",
    "\n",
    "import requests\n",
    "import wikipedia\n",
    "def findWikiData(itemurl):\n",
    "    st = itemurl.find('wiki/')\n",
    "    item = itemurl[st+5:]\n",
    "    #print(item)\n",
    "    s= \"https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&titles=%s&format=json\"%(item)\n",
    "    r = requests.get(s)\n",
    "    rr =r.content.decode(\"utf-8\")\n",
    "    #print('rr=', rr)\n",
    "    ans = rr.find('wikibase_item')\n",
    "    answer = rr[ans+16:]\n",
    "    e = answer.find('\"')\n",
    "    return item, answer[:e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function grabs the instanceOf data (P31) from wikidata.  it returns an empty string if it fails to find anything.  it is basically scraping the json to find the information.  not very elegant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $GOOGLE_APPLICATION_CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInstanceOfId(wikiID):\n",
    "    s = 'https://www.wikidata.org/w/api.php?action=wbgetentities&ids=%s&languages=en&format=json'%(wikiID)\n",
    "    r = requests.get(s)\n",
    "    rr =r.content.decode(\"utf-8\")\n",
    "    try:\n",
    "        j = json.loads(rr)\n",
    "        k = rr.find('P31')\n",
    "        if k > 0:\n",
    "            l = rr[k:].find('\"id\":')\n",
    "            if l > 0:\n",
    "                s = rr[k+l+5:k+l+23]\n",
    "                t = s.find('}')\n",
    "                id = s[1:t-1]\n",
    "                x = id.find('$')\n",
    "                if x < 0:\n",
    "                    return id\n",
    "                else:\n",
    "                    return id[:x]\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            return ''\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getInstanceOfId('Q678023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another wikidata scrape to find the name of an object given it's wikidataID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNameFromWikiID(wikiID):\n",
    "    s = 'https://www.wikidata.org/w/api.php?action=wbgetentities&ids=%s&languages=en&format=json'%(wikiID)\n",
    "    r = requests.get(s)\n",
    "    if r.status_code == 429:\n",
    "        time.sleep(8)\n",
    "        r = requests.get(s)\n",
    "    rr = r.content.decode(\"utf-8\")\n",
    "    s = rr.find('\"value\":')\n",
    "    rrr= rr[s+8:s+50]\n",
    "    t = rrr.find('}')\n",
    "    return rrr[1:t-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getNameFromWikiID(\"Q55814\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDescription(wikiId):\n",
    "    s = 'https://www.wikidata.org/w/api.php?action=wbgetentities&ids=%s&languages=en&format=json'%(wikiId)\n",
    "    r = requests.get(s)\n",
    "    if r.status_code == 429:\n",
    "      time.sleep(8)\n",
    "      r = requests.get(s)\n",
    "\n",
    "    rr =r.content.decode(\"utf-8\")\n",
    "    i = rr.find(\"descriptions\")\n",
    "    if i < 0:\n",
    "        return ''\n",
    "    else:\n",
    "        rrr = rr[i:]\n",
    "        #print(rrr)\n",
    "        j = rrr.find('\"value\":')\n",
    "        if j >0:\n",
    "            #print(j)\n",
    "            k = rrr.find(\"}\")\n",
    "            return(rrr[j+8: k])\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDescription(\"Q55814\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function extracts entities from a sentence.   if it is a wikipedia entity  it pulls some wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityExtractorBlock(sents):\n",
    "    ents = []\n",
    "    x = classify(sents)\n",
    "    for ent in x.entities:\n",
    "        #print('--->', ent.name)\n",
    "        if ent.metadata['wikipedia_url'] != '' or len(str.split(ent.name))> 1:\n",
    "            wikipage = ent.metadata['wikipedia_url']\n",
    "            if wikipage != '':\n",
    "                name, wikidataId = findWikiData(wikipage)\n",
    "                ents.append((name, wikidataId, wikipage))\n",
    "\n",
    "            else:\n",
    "                wikidataId = \"none\"\n",
    "                #print(name, wikidataId, wikipage)\n",
    "                ents.append((ent.name, wikidataId, wikipage))\n",
    "        \n",
    "    return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity extractor creates a list that consists of a list of all of the entities and the last item in the list is the sentence.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityExtractor(sent):\n",
    "    ents = []\n",
    "    newents = entityExtractorBlock(sent)\n",
    "    if newents != []:\n",
    "        entitem = []\n",
    "        for x in newents:\n",
    "            entitem.append({'entity': x})\n",
    "        entitem.append({'context': sent})\n",
    "        ents = ents + entitem\n",
    "    #print(entitem)\n",
    "    return ents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add a new file to the graph.\n",
    "because the articles are named as ArtN to add a new article you need to know the number of articles currently in the graph.  that is art_cnt.\n",
    "\n",
    "for each sentence in the article it invokes the entity extractor.  with the list of entities it creates the entity nodes for each.   for those entities that are in wikipedia it create a list of \"instanceOf\" entities (lightgray in color).  it finally creates an article node.   When this is all done it adds the edges as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFile(art_cnt, G, filename):\n",
    "    items = open('../'+filename+'.txt', 'r').read().replace('\\n', '')\n",
    "    sents = split_into_sentences(items)\n",
    "    for sent in sents:\n",
    "        #print(sent)\n",
    "        ents = entityExtractor(sent)\n",
    "        all_nodes = []\n",
    "        nodes = []\n",
    "        instances = []\n",
    "        ent_list = []\n",
    "        for e in ents:\n",
    "            if e.get('entity') != None:\n",
    "                ent_list.append(e['entity'][0])\n",
    "                #this is an entity for this node\n",
    "                lab = e['entity'][2]\n",
    "                name = e['entity'][0]\n",
    "                wikiID = e['entity'][1]\n",
    "                st = lab.find('wiki/')\n",
    "                if st <0:\n",
    "                    item = name\n",
    "                    #print('Item=', item)\n",
    "                    G.add_node(item, flavor='entity', url='none', wikiID = 'noID', color = 'lightgreen')\n",
    "                    #print(\"just made \", G.node[item])\n",
    "                    G.nodes[item]['instanceof'] = []\n",
    "                    G.nodes[item]['subclassof'] = []\n",
    "                    G.nodes[item]['description'] = ''\n",
    "                    nodes.append(item)\n",
    "\n",
    "                else:\n",
    "                    item = lab[st+5:]\n",
    "                    #print(\"item=\", item)\n",
    "                    #wikiID = findWikiData(lab)[1]\n",
    "                    G.add_node(item, flavor='entity', url=lab, wikiID = wikiID, color = 'lightblue')\n",
    "                    #print(\"just smade \", item, G.nodes[item])\n",
    "                    instID = getInstanceOfId(wikiID)\n",
    "                    if instID != '':\n",
    "                        inst_list = [(instID, getNameFromWikiID(instID))]\n",
    "                        #print([wikiID, (instID, getNameFromWikiID(instID))])\n",
    "                    else:\n",
    "                        inst_list = []\n",
    "                    clas_list = []#isASubclassOf(wikiID)\n",
    "                    G.nodes[item]['instanceof'] = inst_list\n",
    "                    instances = instances + inst_list\n",
    "                    G.nodes[item]['subclassof'] = clas_list        \n",
    "                    #classes = classes + clas_list\n",
    "                    G.nodes[item]['description'] = getDescription(wikiID)\n",
    "                    #print(findWikiData(lab))\n",
    "                    nodes.append(item)\n",
    "            if e.get('context') != None:\n",
    "                # this is a new article\n",
    "                newart = 'Art'+str(art_cnt)\n",
    "                G.add_node(newart, flavor='article', source=filename, context=e['context'], color='lightyellow')\n",
    "                G.nodes[newart]['ent-list'] = ent_list\n",
    "                #print(newart,G.nodes[newart])\n",
    "                art_cnt = art_cnt+1\n",
    "                for z in nodes:\n",
    "                    G.add_edge(newart,z )\n",
    "                all_nodes = all_nodes + nodes\n",
    "                nodes = []\n",
    "        instance_set = set(instances)\n",
    "        instances = list(instance_set)\n",
    "        if instances != []:\n",
    "            print('instances ====', instances)\n",
    "        #now make items for these instances\n",
    "        for i in instances:\n",
    "            G.add_node(i[1], flavor='entity', url='notknown', wikiID = i[0], color = 'lightgray')\n",
    "            G.nodes[i[1]]['description'] = getDescription(i[0])\n",
    "            G.nodes[i[1]]['instanceof'] = []\n",
    "            G.nodes[i[1]]['subclassof'] = []\n",
    "        #now add edges from the instance to the new entity\n",
    "        all_nodes_set = set(all_nodes)\n",
    "        all_nodes =  list(all_nodes_set)\n",
    "        for n in all_nodes:\n",
    "            if G.nodes[n]['instanceof'] != []:\n",
    "                elist = G.nodes[n]['instanceof']\n",
    "                for e in elist:\n",
    "                    G.add_edge(n, e[1])\n",
    "    print(art_cnt)\n",
    "    return art_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the graph from the documents.\n",
    "We are skiping these steps because it is easier to load the graph from the stored graph.  saves lots of time.   If you have your own set of documents you can uncoment this run it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "G = nx.Graph()\n",
    "art_cnt = 0\n",
    "art_cnt =addFile(art_cnt, G, 'relativity')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt, G, 'blackhole')\n",
    "nx.write_gpickle(G, 'phy_plus_geo_singlesent')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt, G, 'blackhole-neutron')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt, G, 'quantum-gravity')\n",
    "nx.write_gpickle(G, 'phy_plus_geo_singlesent')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt, G, 'quantum-grav3')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt, G, 'gravitational-wave')\n",
    "nx.write_gpickle(G, 'phy_plus_geo_singlesent')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt, G, 'string-theory')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt, G, 'cosmology')\n",
    "nx.write_gpickle(G, 'phy_plus_geo_singlesent')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt, G, 'climate')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt,G,\"climate-extinctions\" )\n",
    "nx.write_gpickle(G, 'phy_plus_geo_singlesent')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt,G,'late-devonian')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt,G,'extinction')\n",
    "nx.write_gpickle(G, 'phy_plus_geo_singlesent')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt,G, 'human-extinction')\n",
    "nx.write_gpickle(G, 'phy_plus_geo_singlesent')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "art_cnt = addFile(art_cnt,G, 'climate-change')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line reads the graph file from the saved copy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.write_gpickle(G, 'phy_plus_geo_singlesent')\n",
    "G = nx.read_gpickle('phy_plus_geo_singlesent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will plot the graph.  do not try to plot the entire graph.  too big! \n",
    "but we will use it for subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def showGraph(sg):\n",
    "    options = {\n",
    "        'node_size': 3000,\n",
    "        'font_weight': 'bold',\n",
    "         'edgecolors':'black',\n",
    "        'width': 2,\n",
    "    }\n",
    "    labels = {}\n",
    "    color = []\n",
    "    for n in sg.nodes:\n",
    "        d =sg.nodes[n]\n",
    "        #print(d)\n",
    "        color.append(d['color'])\n",
    "        labels[n]=n\n",
    "    \n",
    "    #print(color)\n",
    "        #print(sg.nodes[x]['url'])\n",
    "        #labels[]\n",
    "        \n",
    "    plt.figure(3,figsize=(15,15)) \n",
    "\n",
    "    nx.draw(sg, labels = labels, node_color = color, **options)   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "showGraph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilities to inspect graph nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showArticle(graph, name):\n",
    "    print(name, graph.nodes[name]['context'])\n",
    "    print('all edges from this node are:')\n",
    "    for x in graph.edges():\n",
    "        if x[1] == name:\n",
    "            print(x, graph.nodes[x[0]]['url'], graph.nodes[x[0]]['wikiID'])\n",
    "            \n",
    "\n",
    "def showEntity(graph, name):\n",
    "    instances = []\n",
    "    classes = []\n",
    "    print(name,' wikiID=', graph.nodes[name]['wikiID'], 'wikipedia url =', graph.nodes[name]['url'])\n",
    "    print(graph.nodes[name]['description'])\n",
    "    inst = graph.nodes[name]['instanceof'] \n",
    "    if inst != []:\n",
    "        instances = instances +inst\n",
    "        ilist = [i[1] for i in inst]\n",
    "        print('is an instance of ', ilist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showEntity(G, 'Cretaceous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showEntity(G, 'geological period')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scorekey function divides the world into 7 categories.   notice relativity is in all the physics categories ... because it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = set(['climate', 'climate-change'])\n",
    "extinction =  set(['late-devonian', 'extinction', \"climate-extinctions\"])\n",
    "human_extinct = set(['human-extinction'])\n",
    "relativity = set(['relativity'])\n",
    "blackholes = set(['relativity','blackhole','blackhole-neutron', 'gravitational-wave'])\n",
    "qgrav = set(['relativity', 'quantum-gravity', 'quantum-grav3', 'string-theory'])\n",
    "cosmo = set(['relativity', 'cosmology'])\n",
    "\n",
    "def scorekey(key1, key2):\n",
    "    if key1 in climate and key2 in climate:\n",
    "        return 1\n",
    "    if key1 in extinction and key2 in extinction:\n",
    "        return 1\n",
    "    if key1 in human_extinct and key2 in human_extinct:\n",
    "        return 1\n",
    "    if key1 in relativity and key2 in relativity:\n",
    "        return 1\n",
    "    if key1 in blackholes and key2 in blackholes:\n",
    "        return 1\n",
    "    if key1 in qgrav and key2 in qgrav:\n",
    "        return 1\n",
    "    if key1 in cosmo and key2 in cosmo:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some book keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = []\n",
    "classes = []\n",
    "lines = []\n",
    "ent_cnt = 0\n",
    "for nd in G.nodes:\n",
    "    #print(G.nodes[nd])\n",
    "    node = G.nodes[nd]\n",
    "    if node['flavor'] == 'entity':\n",
    "        ent_cnt +=1\n",
    "        inst = node['instanceof']\n",
    "        instances = instances + inst\n",
    "        clas = node['subclassof']\n",
    "        classes = classes + clas\n",
    "    elif node['flavor'] == 'article':\n",
    "        #print('doing art ',nd, node)\n",
    "        lines.append([node['context'], nd])\n",
    "instanceSet = set(instances)\n",
    "instances = list(instanceSet)\n",
    "classSet = set(classes)\n",
    "classes = list(classSet)\n",
    "print('instances =', len(instances))\n",
    "print(\"---------------\")\n",
    "print('classes =', len(classes))\n",
    "print('lines =', len(lines))\n",
    "print('entities = ', ent_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [lines[i][0] for i in range(len(lines))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [lines[i][1] for i in range(len(lines))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the BERT embedding\n",
    "and the matrix mar which stores the normalized imbedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = sbert_model.encode(sentences)\n",
    "import numpy as np\n",
    "mar = np.zeros((len(sentences), 768))\n",
    "for i in range(len(sentences)):\n",
    "    x = np.linalg.norm(sentence_embeddings[i])\n",
    "    mar[i] = sentence_embeddings[i]/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_keys(nodeid):\n",
    "    for i in range(len(keys)):\n",
    "        if nodeid == keys[i]:\n",
    "            return i\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function find_best\n",
    "this function takes a sentence and look for the k closest matches based on proximity to the BERT model vectors stored in mar.   This uses dotproduct of normalized vectors which is the same as cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(k, abstract, show=True):\n",
    "    v = sbert_model.encode([abstract])[0]\n",
    "    v0 = v/np.linalg.norm(v)\n",
    "    norms = []\n",
    "    for i in range(mar.shape[0]):\n",
    "        norms.append([np.dot(v0,mar[i]), i])\n",
    "    norms.sort(reverse=True)\n",
    "    if show:\n",
    "        print('top ',k, ' related nodes' )\n",
    "    scor = 0.0\n",
    "    #print(norms[0][1])\n",
    "    nodename = keys[norms[0][1]]\n",
    "    category =  G.nodes[nodename]['source']\n",
    "    for i in range(k):\n",
    "        node = keys[norms[i][1]]\n",
    "        if scorekey(G.nodes[node]['source'],category) == 1:\n",
    "            scor +=1.0\n",
    "        if show:\n",
    "            print(node)\n",
    "            print(sentences[norms[i][1]])\n",
    "    scor = scor/k\n",
    "    return scor, norms[0:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_best3\n",
    "is the same as find_best but we remove candidates that are not in the same connected component.  note: k must be less than 40.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best3(k, abstract, show=True):\n",
    "    v = sbert_model.encode([abstract])[0]\n",
    "    v0 = v/np.linalg.norm(v)\n",
    "    norms = []\n",
    "    for i in range(mar.shape[0]):\n",
    "        norms.append([np.dot(v0,mar[i]), i])\n",
    "    norms.sort(reverse=True)\n",
    "    nodename = keys[norms[0][1]]\n",
    "    newlist = []\n",
    "    nopath = 0\n",
    "    for i in range(40):\n",
    "        #print(norms[i])\n",
    "        x = norms[i][1]\n",
    "        try:\n",
    "            path = nx.shortest_path(G, nodename, 'Art'+str(x))\n",
    "            #print(path)\n",
    "            newlist.append(norms[i])\n",
    "        except:\n",
    "            nopath += 1\n",
    "    norms = newlist\n",
    "    ln = len(norms)\n",
    "    if show:\n",
    "        print('top ',k, ' related nodes' )\n",
    "        if ln < k:\n",
    "            print('but only found ', ln)\n",
    "    scor = 0.0\n",
    "    #print(norms[0][1])\n",
    "    nodename = keys[norms[0][1]]\n",
    "    category =  G.nodes[nodename]['source']\n",
    "    for i in range(min(ln, k)):\n",
    "        node = keys[norms[i][1]]\n",
    "        if scorekey(G.nodes[node]['source'],category) == 1:\n",
    "            scor +=1.0\n",
    "        if show:\n",
    "            print(node)\n",
    "            print(sentences[norms[i][1]])\n",
    "    scor = scor/min(ln, k)\n",
    "    return scor, min(ln, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best3(5, 'Black Holes are hide behind Event horizons.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_best2\n",
    "the function find_best2 is more complicated.  \n",
    "\n",
    "The function findClue extracts entities from the sentence and then looks for the closest match in all of the articles among those that have the same entities.  if there are more than 2 with the max number of matches we just give up and return [].   \n",
    "\n",
    "To compute the convolutions we need to find all the neighbors of each node.  two nodes are neighbors if they share an entity node.   note that in the computation of findNeighbors we exclude two super nodes \"albert_einstein\" and \"earth\".   these node make too many nodes neighbors. \n",
    "\n",
    "ComputeMar2 is the function that computes the convolution matrix mar2.   \n",
    "\n",
    "Function find_best_wl2 is a version of find_best that uses a article nodeID rather than a document to rank near nodes (based on mar2)  it is a utility used in find_best2\n",
    "\n",
    "Function find_best2 looks for clues.  there are either 0, 1 or 2 clues.   if there are 0 clues we use find_best(1 ...) to give us a guess.    if there are 1 or 2 clues we use them in find_best_wl2 to complete the list.  (in the case of 2 clues we use the first clue to generate the first two replies and the second clue to get the rest.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClue(G, sent, show = True):\n",
    "    cls = entityExtractorBlock(sent)\n",
    "    clsent = [x[0] for x in cls]\n",
    "    if show:\n",
    "        print(clsent)\n",
    "    arts = []\n",
    "    for x in cls:\n",
    "        item = x[0]\n",
    "        for edg in G.edges():\n",
    "            #print(edg)\n",
    "            if edg[0] == item and G.nodes[edg[1]]['flavor'] == 'article':\n",
    "                arts.append(edg[1])\n",
    "    artset = set(arts)\n",
    "    artnames = list(artset)\n",
    "    factoredartnames = []\n",
    "    for art in artnames:\n",
    "        #context = G.nodes[art]['context']\n",
    "        #print('context=',context)\n",
    "        cls1ent = G.nodes[art]['ent-list']\n",
    "        #cls1ent = [x[0] for x in cls1]\n",
    "        cls1int = set(clsent).intersection(set(cls1ent))\n",
    "        if cls1int != set():\n",
    "            factoredartnames.append((art, len(cls1int)))\n",
    "        artnames = [x[0] for x in factoredartnames]\n",
    "    #if show:\n",
    "    #    print('artnames=',artnames)\n",
    "    dic = {}\n",
    "    for x in artnames:\n",
    "        dic[x] = 0\n",
    "    for x in factoredartnames:\n",
    "        dic[x[0]] = x[1]\n",
    "    #print('DIC =',dic)\n",
    "    bmax = 0\n",
    "    bestmax = ''\n",
    "    for x in artnames:\n",
    "        if dic[x] > bmax:\n",
    "            bmax = dic[x]\n",
    "            #print(x)\n",
    "            bestmax = x\n",
    "    count_ties = 0\n",
    "    tie_list = []\n",
    "    for x in artnames:\n",
    "        if dic[x] == bmax:\n",
    "            count_ties += 1\n",
    "            tie_list.append(x)\n",
    "    if show:\n",
    "        print(count_ties, 'nodes out of',len(artnames), 'matched the max value of ', bmax)\n",
    "    if count_ties >  0.5*len(artnames) and len(artnames)> 2:\n",
    "        return  []\n",
    "    return tie_list\n",
    "   \n",
    "\n",
    "def findNeighbors(g, art):\n",
    "    n = g.nodes[art]\n",
    "    #print(n)\n",
    "    features = []\n",
    "    neighbors = []\n",
    "    for x in g.edges:\n",
    "        if x[1] == art:\n",
    "            #print('found art in ',x)\n",
    "            p = x[0]\n",
    "            features.append(p)\n",
    "            if p != 'Albert_Einstein' and p != 'Earth':\n",
    "                for y in g.edges:\n",
    "                    if y[0] == p:\n",
    "                        if y[1] != art:\n",
    "                            #print(y[1][0:3])\n",
    "                            if y[1][0:3] != 'Art':\n",
    "                                features.append(y[1])\n",
    "                            else:\n",
    "                                neighbors.append(y[1])\n",
    "    return neighbors, features\n",
    "\n",
    "mar2 = np.zeros((len(sentences), 768))\n",
    "def computeMar2(G, mar, lambd):\n",
    "    num_zero_neighbors = 0\n",
    "    tot_neighbors = 0\n",
    "    num_arts = 0\n",
    "    for art in G.nodes():\n",
    "        if G.nodes[art]['flavor'] == 'article':\n",
    "            num_arts += 1.0\n",
    "            neighbors, features =  findNeighbors(G, art)\n",
    "            v = np.zeros((768))\n",
    "            c = 0.0\n",
    "            if len(neighbors) == 0:\n",
    "                num_zero_neighbors +=1\n",
    "            tot_neighbors += len(neighbors)\n",
    "            if len(neighbors) > 0:\n",
    "                for x in neighbors:\n",
    "                    loc = find_in_keys(x)\n",
    "                    v = v + mar[loc]\n",
    "                    c = c+1.0\n",
    "                v = v/c\n",
    "                z = lambd*mar[find_in_keys(art)] + (1.0-lambd)*v\n",
    "                z = z/np.linalg.norm(z)\n",
    "                mar2[find_in_keys(art)] = z\n",
    "            else:\n",
    "                mar2[find_in_keys(art)] = mar[find_in_keys(art)]\n",
    "\n",
    "    return num_zero_neighbors, tot_neighbors/num_arts\n",
    "\n",
    "\n",
    "def find_best2(k, abstract, show=True):\n",
    "    cls = findClue(G, abstract, show)\n",
    "    #print('clues are =', cls)\n",
    "    if cls != []:\n",
    "        best = cls[0]\n",
    "    else:\n",
    "        _, x = find_best(1, abstract, show=False)\n",
    "        best = keys[x[0][1]]\n",
    "    if(len(cls)> 1):\n",
    "        scora, normsa = find_best_wl2(2, cls[0], show)\n",
    "        scorb, normsb = find_best_wl2(k-2, cls[1], show)\n",
    "        scor = (2*scora+(k-2)*scorb)/k\n",
    "        norms = normsa+normsb\n",
    "    else:\n",
    "        scor, norms = find_best_wl2(k, best, show)\n",
    "    if show:\n",
    "        print('score =', scor)\n",
    "    return scor, norms\n",
    "\n",
    "\n",
    "def find_best_wl2(k, nodeid, show=True):\n",
    "    #print(nodeid)\n",
    "    v0 = mar2[find_in_keys(nodeid)]\n",
    "    norms = []\n",
    "    for i in range(mar2.shape[0]):\n",
    "        norms.append([np.dot(v0,mar2[i]), i])\n",
    "    norms.sort(reverse=True)\n",
    "    score = 0.0\n",
    "    #if show:\n",
    "    #    print('top ',k, ' related nodes' )\n",
    "    scor = 0.0\n",
    "    nodename = keys[norms[0][1]]\n",
    "    category =  G.nodes[nodename]['source']\n",
    "    for i in range(k):\n",
    "        node = keys[norms[i][1]]\n",
    "        sent = sentences[norms[i][1]]\n",
    "        #print('node = ', node)\n",
    "        nodea = node\n",
    "        if scorekey(G.nodes[nodea]['source'],category) == 1:\n",
    "            scor +=1.0\n",
    "        if show:\n",
    "            print(node)\n",
    "            print(sent)\n",
    "    scor = scor/k\n",
    "    return scor, norms[0:k]\n",
    "            \n",
    "def showRelated(G,art):\n",
    "    print( G.nodes[art]['context'])\n",
    "    neighbors, features =  findNeighbors(G, art)\n",
    "    n = set(neighbors)\n",
    "    neighbors = list(n)\n",
    "    print(neighbors)\n",
    "    for x in neighbors:\n",
    "        print( G.nodes[x]['context'])\n",
    "        v0 = mar[find_in_keys(art)]\n",
    "        v1 = mar[find_in_keys(x)]\n",
    "        print(\" dot prod =\", np.dot(v0,v1))\n",
    "    print(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mar2\n",
    "the embedding vectors for the convolution of the BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeMar2(G, mar, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_best2_5 is the same as find_best2 but we eliminate all candidates that are not connected to the top choice.   This is like find_best3 above, and k must be less than 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best2_5(k, text, show = True):\n",
    "    _, norms = find_best2(40, text, show = False)\n",
    "    nodename = keys[norms[0][1]]\n",
    "    newlist = []\n",
    "    nopath = 0\n",
    "    for i in range(40):\n",
    "        #print(norms[i])\n",
    "        x = norms[i][1]\n",
    "        try:\n",
    "            path = nx.shortest_path(G, nodename, 'Art'+str(x))\n",
    "            #print(path)\n",
    "            newlist.append(norms[i])\n",
    "        except:\n",
    "            nopath += 1\n",
    "    norms = newlist\n",
    "    ln = len(norms)\n",
    "\n",
    "    if show:\n",
    "        print('top ',k, ' related nodes' )\n",
    "    scor = 0.0\n",
    "    #print(norms[0][1])\n",
    "    nodename = keys[norms[0][1]]\n",
    "    category =  G.nodes[nodename]['source']\n",
    "    for i in range(min(k,ln)):\n",
    "        node = keys[norms[i][1]]\n",
    "        if scorekey(G.nodes[node]['source'],category) == 1:\n",
    "            scor +=1.0\n",
    "        if show:\n",
    "            print(node)\n",
    "            print(sentences[norms[i][1]])\n",
    "            \n",
    "    scor = scor/min(ln, k)\n",
    "    return scor, min(ln,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'Black Holes are hidden by an Event horizon.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'What is dark energy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'What is dark energy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'unification of general relativity and quantum theory is  supersymmetry known as supergravity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'unification of general relativity and quantum theory is  supersymmetry known as supergravity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'What is M-theory and what does it have to do with string theory and supersymmetry theory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'What is M-theory and what does it have to do with string theory and supersymmetry theory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'Can a neutron star and a black hole merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'Can a neutron star and a black hole merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'How old is the universe and how much of the universe is dark matter and how much is dark energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'How old is the universe and how much of the universe is dark matter and how much is dark energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'what is dark energy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'what is dark energy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4,'mass extinctions were caused by volcanic action and astroid impact.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4,'mass extinctions were caused by volcanic action and astroid impact.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4,'What are the names of the extinction events and what is the most recent.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4,'What are the names of the extinction events and what is the most recent.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'is there a sixth mass extinction?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'is there a sixth mass extinction?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4,'The major cause of climate change is increased'+\n",
    "          ' carbon dioxide levels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4,'The major cause of climate change is increased'+\n",
    "          ' carbon dioxide levels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'Which extinction events were caused by asteroid impacts?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'Which extinction events were caused by asteroid impacts?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'light neutron star')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'light neutron star')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, 'event horizon?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, 'event horizon?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(4, \"who has solved Einstein's field equations?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best2(4, \"who has solved Einstein's field equations?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the \"accuracy\" of the find_best function\n",
    "below we make three measurements of the accuracy by doing \n",
    "find_best(x) for every article in the graph.  \n",
    "\n",
    "recall that the score is not a measure of accuracy, but a measure of how similar the 2nd, 3rd and 4th reply is to the 1st.   \n",
    "\n",
    "find_alone give a score of 70 percent accurate.\n",
    "find_alone2 is better at 75% \n",
    "\n",
    "testing the methods that eliminate answers that are not in the same graph connected component gives even better answers, but it measures the score based on the number it found which may be less than 4. So if it finds no other answers in the component then it gets a score of 1.  \n",
    "\n",
    "find_alone3 is a score of 80% and the average numer of items found is 3.24  \n",
    "find_alone2_5 is a score of 83% and the averege number of items found is 3.33\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artlist = []\n",
    "for n in G.nodes():\n",
    "    if G.nodes[n]['flavor'] == 'article':\n",
    "        artlist.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numarts = len(artlist)\n",
    "print(numarts)\n",
    "scor = 0\n",
    "for n in artlist:\n",
    "    doc = G.nodes[n]['context']\n",
    "    s, _ = find_best(4, doc, show=False)\n",
    "    scor += s\n",
    "print(scor/numarts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numarts = len(artlist)\n",
    "print(numarts)\n",
    "scor = 0\n",
    "num_retured = 0\n",
    "for n in artlist:\n",
    "    doc = G.nodes[n]['context']\n",
    "    #print(doc)\n",
    "    #s, _ = find_best(4, doc[0], show=False)\n",
    "    s,  x = find_best3(4, doc, show=False)\n",
    "    scor += s\n",
    "    num_retured += x\n",
    "print(scor/numarts, num_retured/numarts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numarts = len(artlist)\n",
    "print(numarts)\n",
    "scor = 0\n",
    "for n in artlist:\n",
    "    doc = G.nodes[n]['context']\n",
    "    s, _ = find_best2(4, doc, show=False)\n",
    "    scor += s\n",
    "print(scor/numarts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numarts = len(artlist)\n",
    "print(numarts)\n",
    "scor = 0\n",
    "num_returned = 0\n",
    "for n in artlist:\n",
    "    doc = G.nodes[n]['context']\n",
    "    s, x = find_best2_5(4, doc, show=False)\n",
    "    scor += s\n",
    "    num_returned +=x\n",
    "print(scor/numarts, num_returned/numarts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more layers of convolution\n",
    "computerMar3 applies another layer of convolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMar3(G, mar, lambd):\n",
    "    num_zero_neighbors = 0\n",
    "    tot_neighbors = 0\n",
    "    num_arts = 0\n",
    "    for art in G.nodes():\n",
    "        if G.nodes[art]['flavor'] == 'article':\n",
    "            num_arts += 1.0\n",
    "            neighbors, features =  findNeighbors(G, art)\n",
    "            v = np.zeros((768))\n",
    "            c = 0.0\n",
    "            if len(neighbors) == 0:\n",
    "                num_zero_neighbors +=1\n",
    "            tot_neighbors += len(neighbors)\n",
    "            if len(neighbors) > 0:\n",
    "                for x in neighbors:\n",
    "                    loc = find_in_keys(x)\n",
    "                    v = v + mar[loc]\n",
    "                    c = c+1.0\n",
    "                v = v/c\n",
    "                z = lambd*mar[find_in_keys(art)] + (1.0-lambd)*v\n",
    "                z = z/np.linalg.norm(z)\n",
    "                mar3[find_in_keys(art)] = z\n",
    "            else:\n",
    "                mar3[find_in_keys(art)] = mar[find_in_keys(art)]\n",
    "\n",
    "    return num_zero_neighbors, tot_neighbors/num_arts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar3 = np.zeros((len(sentences), 768))\n",
    "computeMar3(G, mar2,0.8)\n",
    "mar2 = mar3\n",
    "numarts = len(artlist)\n",
    "print(numarts)\n",
    "scor = 0\n",
    "for n in artlist:\n",
    "    s, _ = find_best2(4, n, show=False)\n",
    "    scor += s\n",
    "print(scor/numarts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar3 = np.zeros((len(sentences), 768))\n",
    "computeMar3(G, mar2,0.8)\n",
    "mar2 = mar3\n",
    "numarts = len(artlist)\n",
    "print(numarts)\n",
    "scor = 0\n",
    "for n in artlist:\n",
    "    s, _ = find_best_wl2(4, n, show=False)\n",
    "    scor += s\n",
    "print(scor/numarts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar3 = np.zeros((len(sentences), 768))\n",
    "computeMar3(G, mar2,0.8)\n",
    "mar2 = mar3\n",
    "numarts = len(artlist)\n",
    "print(numarts)\n",
    "scor = 0\n",
    "for n in artlist:\n",
    "    s, _ = find_best_wl2(4, n, show=False)\n",
    "    scor += s\n",
    "print(scor/numarts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the subgraph of nodes associated with a query.\n",
    "this is interesting to see.   it is also interesting to look at the \"story\" that \n",
    "is described by following a path through the subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_trailing_and(text):\n",
    "    i = text.find('and')\n",
    "    if i >0:\n",
    "        i = text.rindex('and')\n",
    "        if len(text)-i < 7:\n",
    "            return text[0:i]\n",
    "        else:\n",
    "            return text\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def explain(sg, entity):\n",
    "    n = sg.nodes[entity]\n",
    "    inst = n['instanceof']\n",
    "    instancefound = False\n",
    "    sent =  entity \n",
    "    #print('=======================================')\n",
    "    if n['description'] != '':\n",
    "        preline = entity+'is defined as ', n['description']\n",
    "    else: \n",
    "        preline = ''\n",
    "    if inst != []:\n",
    "        instancefound = True\n",
    "        sent = sent +' is a '\n",
    "        cnt = 0\n",
    "        for ins in inst:\n",
    "            endline = ''\n",
    "            if cnt < len(inst)-1:\n",
    "                endline = ' and '\n",
    "                cnt = cnt+1\n",
    "            if (ins[1].find('organism') < 0) and (ins[1].find('first-order') < 0) :\n",
    "                sent = sent+ ins[1]+ endline\n",
    "            else:\n",
    "                cnt = cnt-1\n",
    "    sent =  remove_trailing_and(sent)      \n",
    "    inst = n['subclassof']\n",
    "    if inst != []:\n",
    "        if instancefound == True:\n",
    "            sent = sent + ' and '\n",
    "        else:\n",
    "            sent = sent # + ' is '\n",
    "        #sent = sent + ' a subclass of '\n",
    "        #for ins in inst:\n",
    "        #    sent = sent+ ins[1]+' and '\n",
    "   \n",
    "    print(preline, remove_trailing_and(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeArtSubGraph(G, artlist, do_closure = True):\n",
    "    oe = G.edges\n",
    "    sgl = artlist\n",
    "    # compute links from articles to entities\n",
    "    sgl2 = []\n",
    "    for x in oe:\n",
    "        if x[0] in set(sgl):\n",
    "            sgl2.append(x[1])\n",
    "    sgl = sgl + sgl2\n",
    "    sgl = set(sgl)\n",
    "    sgl = list(sgl)\n",
    "    #add links from entities and articles to other things\n",
    "    sgl2 = []\n",
    "    for x in oe:\n",
    "        if x[1] in set(sgl):\n",
    "            sgl2.append(x[0])\n",
    "    sgl = sgl + sgl2\n",
    "    sgl = set(sgl)\n",
    "    sgl = list(sgl)\n",
    "    \n",
    "    if do_closure:\n",
    "        sgl2 = []\n",
    "        for x in oe:\n",
    "            if x[0] in set(sgl):\n",
    "                sgl2.append(x[1])\n",
    "        sgl = sgl + sgl2\n",
    "        sgl = set(sgl)\n",
    "        sgl = list(sgl)\n",
    "\n",
    "    sglab = {}\n",
    "    for x in sgl:\n",
    "        sglab[x] = x\n",
    "    sg = nx.subgraph(G, sgl)\n",
    "    return sglab, sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMatchingGraph(G, text , do_closure = True):\n",
    "    _, s = find_best2(5, text, show=False)\n",
    "    print('s=',s)\n",
    "    artlist = [keys[it[1]] for it in s]\n",
    "    print(len(artlist))\n",
    "    for state in artlist:\n",
    "        print(state+'--', G.nodes[state]['context'])\n",
    "    _, sg = makeArtSubGraph(G, artlist, do_closure)\n",
    "    showGraph(sg)\n",
    "    print(\"related entities are:\")\n",
    "    print('=====================================')\n",
    "    for nd in sg:\n",
    "        n = sg.nodes[nd]\n",
    "        if n['flavor']== 'entity':\n",
    "            explain(sg, nd)\n",
    "            print(\" \")\n",
    "    return sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainPath(cop, X, Y):\n",
    "    path = nx.shortest_path(cop, X, Y)\n",
    "    print(path)\n",
    "    for a in path:\n",
    "        print(\"----------------\")\n",
    "        if cop.nodes[a]['flavor'] == \"entity\":\n",
    "            print(a, cop.nodes[a]['description'])\n",
    "        if cop.nodes[a]['flavor'] == 'article':\n",
    "            print(a, cop.nodes[a]['context'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = buildMatchingGraph(G,'best-known cause of a mass extinction is an Asteroid '+\n",
    "                        'impact that killed off the Dinosaurs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainPath(sg,  'supernova explosions', 'Art407')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainPath(sg,  'Art273', 'Yucatán_Peninsula')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg =buildMatchingGraph(G,\"What is dark energy?\", do_closure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's compute the average number of neighbors a node has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numarts = len(artlist)\n",
    "print(numarts)\n",
    "scor = 0\n",
    "neighbors = np.zeros(5)\n",
    "for n in artlist:\n",
    "    nbrs, _ = findNeighbors(G, n)\n",
    "    l = len(nbrs)\n",
    "    if l > 4:\n",
    "        l = 4\n",
    "    neighbors[l]+=1\n",
    "print(neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for i in range(5):\n",
    "    tot += neighbors[i]*i\n",
    "tot/numarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute neighbors of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numarts = len(artlist)\n",
    "print(numarts)\n",
    "scor = 0\n",
    "neighbors = np.zeros(5)\n",
    "for n in artlist:\n",
    "    nbrs, _ = findNeighbors(G, n)\n",
    "    dubnbrs = []\n",
    "    for x in nbrs:\n",
    "        if x != n:\n",
    "            dub, _ = findNeighbors(G, x)\n",
    "            dubnbrs = dubnbrs + dub\n",
    "    sdubnbrs = set(dubnbrs)\n",
    "    nbrs = list(sdubnbrs)\n",
    "    l = len(nbrs)\n",
    "    if l > 4:\n",
    "        l = 4\n",
    "    neighbors[l]+=1\n",
    "print(neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for i in range(5):\n",
    "    tot += neighbors[i]*i\n",
    "tot/numarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "140/427"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one third of the nodes have no neighbors.  These nodes are not effected by the convolution operation.\n",
    "\n",
    "now  look at the sizes of the connected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = nx.connected_components(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comp = 0\n",
    "comp_size = np.zeros(7)\n",
    "for x in comp:\n",
    "    x_size =  0\n",
    "    for n in x:\n",
    "        if G.node[n]['flavor'] == 'article':\n",
    "            x_size += 1\n",
    "    if x_size > 6:\n",
    "        comp_size[6] = x_size\n",
    "    else:\n",
    "        comp_size[x_size] += 1\n",
    "print(comp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nodes = 0\n",
    "for i in range(6):\n",
    "    total_nodes += comp_size[i]*i\n",
    "total_nodes + 304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = (76+10*2 +3*3 +4*(2+2+304))/427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
